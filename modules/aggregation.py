import pandas as pd
import io
from datetime import datetime
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

# å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å…±é€šã§å«ã¾ã‚Œã‚‹å›ºå®šè³ªå•
FIXED_QUESTIONS = [
    'ã‚ãªãŸã®å¹´ä»£æ€§åˆ¥ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸãŒãŠä½ã¾ã„ã®éƒ½é“åºœçœŒã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚',
    'å®¶æ—æ§‹æˆã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã®å¹´åã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ç¤¾ä¼šäººçµŒé¨“ã¯ä½•å¹´é–“ã§ã™ã‹ï¼Ÿ',
    'æœ€çµ‚å­¦æ­´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã®ãŠä½ã„ã®å®¶è³ƒã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã«å½“ã¦ã¯ã¾ã‚‹é¸æŠè‚¢ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚'
]

def extract_question_mapping_from_survey(uploaded_file):
    """
    ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®è³ªå•å¯¾å¿œè¡¨ã‚·ãƒ¼ãƒˆã‹ã‚‰è³ªå•ã¨ãã®é¸æŠè‚¢ã‚’æŠ½å‡ºã™ã‚‹

    Args:
        uploaded_file: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        list: è³ªå•å¯¾å¿œè¡¨ã¨åŒã˜å½¢å¼ã®è¾æ›¸ã®ãƒªã‚¹ãƒˆ
              [{'ç•ªå·': 'Q-001', 'æ¡ä»¶': 'å¿…é ˆå›ç­”', 'å†…å®¹': 'è³ªå•æ–‡', 'åŒºåˆ†': 'S/A'},
               {'ç•ªå·': '1', 'æ¡ä»¶': '', 'å†…å®¹': 'é¸æŠè‚¢1', 'åŒºåˆ†': ''}, ...]
    """
    try:
        # è³ªå•å¯¾å¿œè¡¨ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        xl_file = pd.ExcelFile(uploaded_file)
        if 'è³ªå•å¯¾å¿œè¡¨' not in xl_file.sheet_names:
            return []

        # è³ªå•å¯¾å¿œè¡¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        question_df = pd.read_excel(uploaded_file, sheet_name='è³ªå•å¯¾å¿œè¡¨', header=1)

        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        mapping_data = []
        for _, row in question_df.iterrows():
            # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if pd.isna(row.iloc[0]) and pd.isna(row.iloc[2]):
                continue

            # 4åˆ—ã®æ§‹é€ ã«å¤‰æ›
            entry = {
                'ç•ªå·': str(row.iloc[0]) if pd.notna(row.iloc[0]) else '',
                'æ¡ä»¶': str(row.iloc[1]) if pd.notna(row.iloc[1]) else '',
                'å†…å®¹': str(row.iloc[2]) if pd.notna(row.iloc[2]) else '',
                'åŒºåˆ†': str(row.iloc[3]) if pd.notna(row.iloc[3]) else ''
            }
            mapping_data.append(entry)

        return mapping_data

    except Exception as e:
        logging.warning(f"è³ªå•å¯¾å¿œè¡¨ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return []


def aggregate_data(data_files, question_master_df, client_settings_df):
    """
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã«åŸºã¥ãã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆã—ã€
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã”ã¨ã«å€‹åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦è¿”ã™ã€‚
    
    Args:
        data_files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        question_master_df: è³ªå•ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        client_settings_df: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
        dict: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        pandas.DataFrame: ä¸­é–“ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨çµåˆãƒ‡ãƒ¼ã‚¿ï¼‰
        list: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    if not data_files:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å°‘ãªãã¨ã‚‚1ã¤ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

    logging.info(f"Received {len(data_files)} data files for aggregation")
    logs = []
    all_data_list = []

    logs.append("--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å¤‰æ›å‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files)}")
    for i, f in enumerate(data_files):
        file_size = getattr(f, 'size', 'unknown')
        logs.append(f"  {i+1}. {f.name} (ã‚µã‚¤ã‚º: {file_size} bytes)")

    # ğŸ†• è³ªå•å¯¾å¿œè¡¨ã®åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    comprehensive_question_mapping = []

    for uploaded_file in data_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ–‡å­—åŒ–ã‘å¯¾ç­–ï¼ˆquestion_master.pyã¨åŒã˜å‡¦ç†ï¼‰
        original_filename = uploaded_file.name
        filename = original_filename
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
        logging.info(f"Original filename in aggregation: {repr(original_filename)}")
        
        # æ–‡å­—åŒ–ã‘ã®æ¤œå‡ºã¨ä¿®æ­£
        try:
            # ä¸€èˆ¬çš„ãªæ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if any(ord(c) > 127 and ord(c) < 256 for c in filename):
                # Latin-1ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆ
                try:
                    filename = filename.encode('latin-1').decode('utf-8')
                except:
                    pass
            
            # ãã‚Œã§ã‚‚æ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹å ´åˆã¯ã€å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            if 'ï¿½' in filename or any(ord(c) > 0xFFFF for c in filename):
                import hashlib
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ç”¨
                file_hash = hashlib.md5(uploaded_file.name.encode('utf-8', errors='ignore')).hexdigest()[:8]
                filename = f"file_{file_hash}.xlsx"
                logging.warning(f"Filename contains invalid characters in aggregation. Using safe name: {filename}")
        except Exception:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä½¿ç”¨
            import time
            filename = f"file_{int(time.time())}.xlsx"
            
        if filename.endswith('.xlsx') and not filename.startswith('~'):
            logs.append(f"'{filename}' ã®å‡¦ç†ã‚’é–‹å§‹...")
            
            # question_master_dfã®åˆ—ã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã®åˆ—ã‚’æ¢ã™
            # æ–‡å­—åŒ–ã‘ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã¨ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
            file_column = None
            q_to_text_map = {}

            # 1. å®Œå…¨ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
            if filename in question_master_df.columns:
                file_column = filename
            elif original_filename in question_master_df.columns:
                file_column = original_filename
            # 2. Plus_ãƒãƒ¼ã‚¸å®Œæˆãƒ‡ãƒ¼ã‚¿.xlsx ã®ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹
            elif "Plus_ãƒãƒ¼ã‚¸å®Œæˆãƒ‡ãƒ¼ã‚¿" in filename:
                for col in question_master_df.columns:
                    if "Plus2" in col and not col.startswith("[ã‚³ãƒ”ãƒ¼]"):
                        file_column = col
                        break
            # 3. éƒ¨åˆ†ä¸€è‡´ã§ã®æ¤œç´¢ï¼ˆfallbackï¼‰
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸»è¦éƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦æ¤œç´¢
                clean_filename = filename.replace(".xlsx", "")
                for col in question_master_df.columns:
                    if col.endswith('.xlsx') and clean_filename in col:
                        file_column = col
                        break

            if file_column:
                file_mapping = question_master_df[['è³ªå•æ–‡', file_column]].dropna()
                q_to_text_map = dict(zip(file_mapping[file_column], file_mapping['è³ªå•æ–‡']))
                logs.append(f"'{filename}' ã®è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—ã—ã¾ã—ãŸã€‚({len(q_to_text_map)}å€‹ã®è³ªå•)")
            else:
                # å…·ä½“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹
                # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åé›†
                logs.append(f"'{filename}' (å…ƒ: '{original_filename}') ã«å¯¾å¿œã™ã‚‹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                logs.append(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {[col for col in question_master_df.columns if col.endswith('.xlsx')]}")

                # æ±ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã—ã¦æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨
                first_file_col = None
                for col in question_master_df.columns:
                    if col != 'è³ªå•æ–‡' and col.endswith('.xlsx'):
                        first_file_col = col
                        break

                if first_file_col:
                    temp_mapping = question_master_df[['è³ªå•æ–‡', first_file_col]].dropna()
                    q_to_text_map = dict(zip(temp_mapping[first_file_col], temp_mapping['è³ªå•æ–‡']))
                    logs.append(f"'{filename}' ã§ã¯ '{first_file_col}' ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä»£æ›¿ä½¿ç”¨ã—ã¾ã™ã€‚({len(q_to_text_map)}å€‹ã®è³ªå•)")
                else:
                    logs.append(f"'{filename}' ã§ã¯åˆ©ç”¨å¯èƒ½ãªãƒãƒƒãƒ”ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ƒã®åˆ—åã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

                logging.warning(f"File column not found for {filename} or {original_filename}. Using generic mapping from {first_file_col}.")
                logging.info(f"Available columns: {list(question_master_df.columns)}")
            
            try:
                df_data = pd.read_excel(uploaded_file, sheet_name='data')
                if df_data.empty:
                    logs.append(f"'{filename}' ã®dataã‚·ãƒ¼ãƒˆã¯ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ğŸ†• ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®è³ªå•å¯¾å¿œè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦è¿½åŠ 
                file_question_mapping = extract_question_mapping_from_survey(uploaded_file)
                if file_question_mapping:
                    comprehensive_question_mapping.extend(file_question_mapping)
                    logs.append(f"'{filename}' ã‹ã‚‰ {len(file_question_mapping)} è¡Œã®è³ªå•å¯¾å¿œè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")

                new_columns = {}
                for col in df_data.columns:
                    if col in q_to_text_map:
                        new_columns[col] = q_to_text_map[col]
                    else:
                        for q_num, q_text in q_to_text_map.items():
                            if str(col).startswith(q_num + '_'):
                                suffix = str(col).replace(q_num, '')
                                new_columns[col] = f"{q_text}{suffix}"
                                break
                df_data.rename(columns=new_columns, inplace=True)
                
                all_data_list.append(df_data)
                logs.append(f"'{filename}' ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†ã€‚({len(df_data)}ä»¶)")

            except Exception as e:
                logs.append(f"'{filename}' ã®ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    if not all_data_list:
        raise ValueError("é›†è¨ˆå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    logs.append("--- å…¨ãƒ‡ãƒ¼ã‚¿ã®çµåˆå‡¦ç†ã‚’é–‹å§‹ ---")
    merged_df = pd.concat(all_data_list, ignore_index=True, sort=False)
    logs.append(f"å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸã€‚åˆè¨ˆ: {len(merged_df)}ä»¶")

    if 'å›ç­”æ—¥æ™‚' in merged_df.columns:
        merged_df['å›ç­”æ—¥æ™‚'] = pd.to_datetime(merged_df['å›ç­”æ—¥æ™‚'], errors='coerce')
        merged_df.dropna(subset=['å›ç­”æ—¥æ™‚'], inplace=True)
        merged_df.sort_values(by='å›ç­”æ—¥æ™‚', inplace=True)
        logs.append(f"å›ç­”æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥ã®é›†è¨ˆ
    client_results = {}
    logs.append("--- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥é›†è¨ˆå‡¦ç†ã‚’é–‹å§‹ ---")
    
    for client_name, group in client_settings_df.groupby('ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå'):
        logs.append(f"'{client_name}' ã®é›†è¨ˆã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã‹ã‚‰è³ªå•ã‚’å–å¾—
        questions_to_aggregate = group['é›†è¨ˆå¯¾è±¡ã®è³ªå•æ–‡'].tolist()
        
        # å›ºå®šè³ªå•ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é™¤å¤–ï¼‰
        all_questions = list(dict.fromkeys(FIXED_QUESTIONS + questions_to_aggregate))
        logs.append(f"'{client_name}' ã«ã¯å›ºå®šè³ªå•ã‚’å«ã‚€åˆè¨ˆ {len(all_questions)} å€‹ã®è³ªå•ã‚’é›†è¨ˆã—ã¾ã™ã€‚")
        
        cols_to_select = ['NO']
        for q in all_questions:
            if q in merged_df.columns:
                cols_to_select.append(q)
            for col in merged_df.columns:
                if str(col).startswith(q + '_'):
                    cols_to_select.append(col)
        
        cols_to_select = list(dict.fromkeys(cols_to_select))
        
        if 'å›ç­”æ—¥æ™‚' in merged_df.columns:
            cols_to_select.append('å›ç­”æ—¥æ™‚')
        
        if len(cols_to_select) <= 1:
            logs.append(f"'{client_name}' ã®é›†è¨ˆå¯¾è±¡ã®è³ªå•ãŒãƒ‡ãƒ¼ã‚¿å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue
            
        client_data = merged_df[cols_to_select]
        
        base_mapping_df = pd.DataFrame()
        text_to_q_map = {}
        
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åé›†ï¼ˆå›ºå®šè³ªå•ã‚’å«ã‚€å…¨è³ªå•ã‚’ç¢ºå®Ÿã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        for file_col in question_master_df.columns:
            if file_col != 'è³ªå•æ–‡' and file_col.endswith('.xlsx'):
                temp_mapping = question_master_df[['è³ªå•æ–‡', file_col]].dropna()
                for _, row in temp_mapping.iterrows():
                    if row['è³ªå•æ–‡'] not in text_to_q_map:
                        text_to_q_map[row['è³ªå•æ–‡']] = row[file_col]
        
        # ğŸ†• è³ªå•å¯¾å¿œè¡¨å½¢å¼ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆè³ªå• + é¸æŠè‚¢ã‚’å«ã‚€ï¼‰
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ãƒªã‚¹ãƒˆã«è©²å½“ã™ã‚‹è³ªå•å¯¾å¿œè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        client_mapping_data = []

        for question_text in all_questions:
            # comprehensive_question_mapping ã‹ã‚‰è©²å½“ã™ã‚‹è³ªå•ã¨ãã®é¸æŠè‚¢ã‚’æ¢ã™
            question_found = False
            for mapping_entry in comprehensive_question_mapping:
                # è³ªå•ã®ãƒ¡ã‚¤ãƒ³è¡Œã‚’æ¢ã™ï¼ˆç•ªå·ãŒQ-ã§å§‹ã¾ã‚Šã€å†…å®¹ãŒè³ªå•æ–‡ã¨ä¸€è‡´ï¼‰
                if (mapping_entry['ç•ªå·'].startswith('Q-') and
                    mapping_entry['å†…å®¹'] == question_text):

                    # è³ªå•ã®ãƒ¡ã‚¤ãƒ³è¡Œã‚’è¿½åŠ 
                    client_mapping_data.append(mapping_entry)
                    question_found = True

                    # ã“ã®è³ªå•ã®é¸æŠè‚¢ã‚‚æ¢ã—ã¦è¿½åŠ 
                    question_index = comprehensive_question_mapping.index(mapping_entry)

                    # è³ªå•ã®ç›´å¾Œã‹ã‚‰æ¬¡ã®è³ªå•ã¾ã§ã®é¸æŠè‚¢ã‚’åé›†
                    for i in range(question_index + 1, len(comprehensive_question_mapping)):
                        choice_entry = comprehensive_question_mapping[i]

                        # æ¬¡ã®è³ªå•ï¼ˆQ-ã§å§‹ã¾ã‚‹ï¼‰ãŒè¦‹ã¤ã‹ã£ãŸã‚‰åœæ­¢
                        if choice_entry['ç•ªå·'].startswith('Q-'):
                            break

                        # é¸æŠè‚¢ï¼ˆæ•°å­—ã®ç•ªå·ã§å†…å®¹ãŒã‚ã‚‹ï¼‰ã‚’è¿½åŠ 
                        if (choice_entry['ç•ªå·'].isdigit() and
                            choice_entry['å†…å®¹'].strip()):
                            client_mapping_data.append(choice_entry)

                    break

            # ã‚‚ã—è³ªå•å¯¾å¿œè¡¨ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å¾“æ¥ã®æ–¹æ³•ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not question_found:
                matching_rows = question_master_df[question_master_df['è³ªå•æ–‡'] == question_text]
                if not matching_rows.empty:
                    row = matching_rows.iloc[0]
                    for col in question_master_df.columns:
                        if col.endswith('.xlsx') and pd.notna(row[col]):
                            client_mapping_data.append({
                                'ç•ªå·': row[col],
                                'æ¡ä»¶': '',
                                'å†…å®¹': question_text,
                                'åŒºåˆ†': ''
                            })
                            break

        # DataFrameã‚’ä½œæˆ
        if client_mapping_data:
            base_mapping_df = pd.DataFrame(client_mapping_data)
            # åˆ—ã®é †åºã‚’èª¿æ•´
            base_mapping_df = base_mapping_df[['ç•ªå·', 'æ¡ä»¶', 'å†…å®¹', 'åŒºåˆ†']]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç©ºã®DataFrame
            base_mapping_df = pd.DataFrame(columns=['ç•ªå·', 'æ¡ä»¶', 'å†…å®¹', 'åŒºåˆ†'])

        logs.append(f"'{client_name}' ã®ãƒãƒƒãƒ”ãƒ³ã‚°: {len(base_mapping_df)}è¡Œï¼ˆè³ªå•+é¸æŠè‚¢ã‚’å«ã‚€ï¼‰")

        # client_dataã®åˆ—åã‚’è³ªå•æ–‡ã‹ã‚‰è³ªå•ç•ªå·ã¸å†å¤‰æ›ï¼ˆFAåˆ—ã‚‚è€ƒæ…®ï¼‰
        final_rename_map = {}
        for col_name in client_data.columns:
            # FAåˆ—ãªã©ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒä»˜ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_suffixed = False
            for q_text, q_num in text_to_q_map.items():
                if str(col_name).startswith(q_text + '_'):
                    suffix = str(col_name).replace(q_text, '')
                    final_rename_map[col_name] = q_num + suffix
                    is_suffixed = True
                    break
            # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒãªãã€å®Œå…¨ä¸€è‡´ã™ã‚‹å ´åˆ
            if not is_suffixed and col_name in text_to_q_map:
                final_rename_map[col_name] = text_to_q_map[col_name]

        output_client_data = client_data.rename(columns=final_rename_map)
        
        client_results[client_name] = {
            'data': output_client_data,
            'base_file': f"{client_name}å°‚ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°",
            'mapping': base_mapping_df
        }
        
        logs.append(f"'{client_name}' ã®é›†è¨ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    return client_results, merged_df, logs