import pandas as pd
import io
from datetime import datetime
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

# å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å…±é€šã§å«ã¾ã‚Œã‚‹å›ºå®šè³ªå•
FIXED_QUESTIONS = [
    'ã‚ãªãŸã®å¹´ä»£æ€§åˆ¥ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸãŒãŠä½ã¾ã„ã®éƒ½é“åºœçœŒã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚',
    'å®¶æ—æ§‹æˆã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã®å¹´åã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ç¤¾ä¼šäººçµŒé¨“ã¯ä½•å¹´é–“ã§ã™ã‹ï¼Ÿ',
    'æœ€çµ‚å­¦æ­´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã®ãŠä½ã„ã®å®¶è³ƒã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã«å½“ã¦ã¯ã¾ã‚‹é¸æŠè‚¢ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚'
]

def aggregate_data(data_files, question_master_df, client_settings_df):
    """
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã«åŸºã¥ãã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆã—ã€
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã”ã¨ã«å€‹åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦è¿”ã™ã€‚
    
    Args:
        data_files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        question_master_df: è³ªå•ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        client_settings_df: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
        dict: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        pandas.DataFrame: ä¸­é–“ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨çµåˆãƒ‡ãƒ¼ã‚¿ï¼‰
        list: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    # Debug print to console and file
    debug_msg = f"\n=== AGGREGATE_DATA CALLED ===\n"
    debug_msg += f"Number of data_files received: {len(data_files)}\n"
    for i, f in enumerate(data_files):
        debug_msg += f"  File {i+1}: {f.name} (size: {f.size} bytes)\n"
    debug_msg += "=" * 30 + "\n"
    
    print(debug_msg)
    
    # Also write to file for debugging
    with open(r"D:\python\tri-merger\debug_log.txt", "w", encoding="utf-8") as f:
        f.write(debug_msg)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    if not data_files:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å°‘ãªãã¨ã‚‚1ã¤ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    
    logging.info(f"Received {len(data_files)} data files for aggregation")
    logs = []
    all_data_list = []
    
    logs.append("--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å¤‰æ›å‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files)}")
    for i, f in enumerate(data_files):
        logs.append(f"  {i+1}. {f.name} (ã‚µã‚¤ã‚º: {f.size} bytes)")
    
    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆ90å€‹ã®è³ªå•ã™ã¹ã¦ã‚’ã‚«ãƒãƒ¼ï¼‰
    global_q_to_text_map = {}
    for col in question_master_df.columns:
        if col != 'è³ªå•æ–‡' and col != 'åˆå‡ºãƒ•ã‚¡ã‚¤ãƒ«' and col.endswith('.xlsx'):
            temp_mapping = question_master_df[['è³ªå•æ–‡', col]].dropna()
            for _, row in temp_mapping.iterrows():
                # ã¾ã ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„è³ªå•ã®ã¿è¿½åŠ 
                if row[col] not in global_q_to_text_map:
                    global_q_to_text_map[row[col]] = row['è³ªå•æ–‡']
    
    logs.append(f"çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ: {len(global_q_to_text_map)}å€‹ã®è³ªå•ã‚³ãƒ¼ãƒ‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒãƒƒãƒ”ãƒ³ã‚°")
    logs.append(f"question_masterã‹ã‚‰æ¤œå‡ºã•ã‚ŒãŸç·è³ªå•æ•°: {len(question_master_df)}å€‹")
    
    print(f"ğŸ” DEBUG: Starting to process {len(data_files)} files")

    for uploaded_file in data_files:
        print(f"ğŸ” DEBUG: Processing file: {uploaded_file.name}")

        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ–‡å­—åŒ–ã‘å¯¾ç­–ï¼ˆquestion_master.pyã¨åŒã˜å‡¦ç†ï¼‰
        original_filename = uploaded_file.name
        filename = original_filename

        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
        logging.info(f"Original filename in aggregation: {repr(original_filename)}")
        print(f"ğŸ” DEBUG: Original filename: {repr(original_filename)}")
        
        # æ–‡å­—åŒ–ã‘ã®æ¤œå‡ºã¨ä¿®æ­£
        try:
            # ä¸€èˆ¬çš„ãªæ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if any(ord(c) > 127 and ord(c) < 256 for c in filename):
                # Latin-1ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆ
                try:
                    filename = filename.encode('latin-1').decode('utf-8')
                except:
                    pass
            
            # ãã‚Œã§ã‚‚æ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹å ´åˆã¯ã€å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            if 'ï¿½' in filename or any(ord(c) > 0xFFFF for c in filename):
                import hashlib
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ç”¨
                file_hash = hashlib.md5(uploaded_file.name.encode('utf-8', errors='ignore')).hexdigest()[:8]
                filename = f"file_{file_hash}.xlsx"
                logging.warning(f"Filename contains invalid characters in aggregation. Using safe name: {filename}")
        except Exception:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä½¿ç”¨
            import time
            filename = f"file_{int(time.time())}.xlsx"
            
        if filename.endswith('.xlsx') and not filename.startswith('~'):
            logs.append(f"'{filename}' ã®å‡¦ç†ã‚’é–‹å§‹...")
            
            # question_master_dfã®åˆ—ã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã®åˆ—ã‚’æ¢ã™
            # æ–‡å­—åŒ–ã‘ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã¨ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
            file_column = None
            q_to_text_map = {}
            
            if filename in question_master_df.columns:
                file_column = filename
            elif original_filename in question_master_df.columns:
                file_column = original_filename
            
            if file_column:
                file_mapping = question_master_df[['è³ªå•æ–‡', file_column]].dropna()
                q_to_text_map = dict(zip(file_mapping[file_column], file_mapping['è³ªå•æ–‡']))
                logs.append(f"'{filename}' ã®è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—ã—ã¾ã—ãŸã€‚({len(q_to_text_map)}å€‹ã®è³ªå•)")
            else:
                # å…·ä½“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹
                # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åé›†
                logs.append(f"'{filename}' (å…ƒ: '{original_filename}') ã«å¯¾å¿œã™ã‚‹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                logs.append(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {[col for col in question_master_df.columns if col.endswith('.xlsx')]}")
                
                # æ±ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã—ã¦æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨
                first_file_col = None
                for col in question_master_df.columns:
                    if col != 'è³ªå•æ–‡' and col.endswith('.xlsx'):
                        first_file_col = col
                        break
                
                if first_file_col:
                    temp_mapping = question_master_df[['è³ªå•æ–‡', first_file_col]].dropna()
                    q_to_text_map = dict(zip(temp_mapping[first_file_col], temp_mapping['è³ªå•æ–‡']))
                    logs.append(f"'{filename}' ã§ã¯ '{first_file_col}' ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä»£æ›¿ä½¿ç”¨ã—ã¾ã™ã€‚({len(q_to_text_map)}å€‹ã®è³ªå•)")
                else:
                    logs.append(f"'{filename}' ã§ã¯åˆ©ç”¨å¯èƒ½ãªãƒãƒƒãƒ”ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ƒã®åˆ—åã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                
                logging.warning(f"File column not found for {filename} or {original_filename}. Using generic mapping from {first_file_col}.")
                logging.info(f"Available columns: {list(question_master_df.columns)}")
            
            try:
                print(f"ğŸ” DEBUG: About to read Excel file: {filename}")
                df_data = pd.read_excel(uploaded_file, sheet_name='data')
                print(f"âœ… DEBUG: Successfully read Excel: {len(df_data)} rows, {len(df_data.columns)} columns")
                logs.append(f"'{filename}' ã®dataã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€‚å…ƒãƒ‡ãƒ¼ã‚¿: {len(df_data)}è¡Œ, {len(df_data.columns)}åˆ—")
                
                if df_data.empty:
                    logs.append(f"'{filename}' ã®dataã‚·ãƒ¼ãƒˆã¯ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®åˆ—ã‚’rename
                new_columns = {}
                mapped_count = 0
                for col in df_data.columns:
                    if col in global_q_to_text_map:
                        new_columns[col] = global_q_to_text_map[col]
                        mapped_count += 1
                    else:
                        # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä¾‹: Q-001_1, Q-001_2ãªã©ï¼‰
                        for q_num, q_text in global_q_to_text_map.items():
                            if str(col).startswith(q_num + '_'):
                                suffix = str(col).replace(q_num, '')
                                new_columns[col] = f"{q_text}{suffix}"
                                mapped_count += 1
                                break
                
                logs.append(f"'{filename}' ã§ {mapped_count}/{len(df_data.columns)} åˆ—ãŒãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸã€‚")
                df_data.rename(columns=new_columns, inplace=True)
                
                all_data_list.append(df_data)
                logs.append(f"'{filename}' ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†ã€‚({len(df_data)}ä»¶) -> all_data_liståˆè¨ˆ: {len(all_data_list)}ãƒ•ã‚¡ã‚¤ãƒ«")

            except Exception as e:
                print(f"âŒ DEBUG: Error processing {filename}: {e}")
                import traceback
                error_traceback = traceback.format_exc()
                print(f"âŒ DEBUG: Full traceback:\n{error_traceback}")
                logs.append(f"'{filename}' ã®ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                logs.append(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_traceback}")

    print(f"ğŸ” DEBUG: Checking data files processing results")
    print(f"   all_data_list length: {len(all_data_list)}")
    print(f"   Original data_files count: {len(data_files)}")

    if not all_data_list:
        print(f"âŒ NO DATA FOUND! This is the error, not reindexing!")
        print(f"   Files were processed but no data sheets were successfully read")
        print(f"   Check the logs above for file processing errors")
        raise ValueError("é›†è¨ˆå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    logs.append("--- å…¨ãƒ‡ãƒ¼ã‚¿ã®çµåˆå‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"çµåˆå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_data_list)}")
    for i, df in enumerate(all_data_list):
        logs.append(f"  ãƒ•ã‚¡ã‚¤ãƒ«{i+1}: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    
    # Fix duplicate columns before concat to prevent reindexing errors
    def fix_duplicate_columns_for_concat(dfs_list):
        """Fix duplicate column names across dataframes before concat"""
        if len(dfs_list) <= 1:
            return dfs_list

        print(f"ğŸ” Checking for duplicate columns across {len(dfs_list)} dataframes")

        # Collect all column names from all dataframes
        all_columns = set()
        df_columns = []

        for i, df in enumerate(dfs_list):
            cols = list(df.columns)
            df_columns.append(cols)
            all_columns.update(cols)
            print(f"   DataFrame {i+1}: {len(cols)} columns")

            # Check for duplicates within the same dataframe
            if df.columns.duplicated().any():
                print(f"   âš ï¸ DataFrame {i+1} has internal duplicate columns")

        # Find columns that appear in multiple dataframes
        column_counts = {}
        for i, cols in enumerate(df_columns):
            for col in cols:
                if col not in column_counts:
                    column_counts[col] = []
                column_counts[col].append(i)

        # Identify problematic columns (appear in multiple dataframes)
        problematic_columns = {col: dfs for col, dfs in column_counts.items() if len(dfs) > 1}

        if problematic_columns:
            print(f"   âš ï¸ Found {len(problematic_columns)} columns appearing in multiple dataframes")
            for col, df_indices in list(problematic_columns.items())[:5]:  # Show first 5
                print(f"     '{col}' appears in dataframes: {df_indices}")

        # Create union of all columns for consistent structure
        all_columns_list = sorted(list(all_columns))
        print(f"   Total unique columns across all dataframes: {len(all_columns_list)}")

        # Reindex all dataframes to have the same columns
        fixed_dfs = []
        for i, df in enumerate(dfs_list):
            print(f"   Reindexing DataFrame {i+1}...")

            # Fix duplicate column names first - this is the core issue!
            df_copy = df.copy()
            if df_copy.columns.duplicated().any():
                print(f"     âš ï¸ Found {df_copy.columns.duplicated().sum()} duplicate column names, fixing...")
                # Make column names unique by adding suffixes using pandas built-in method
                cols = pd.io.common.dedup_names(df_copy.columns, is_potential_multiindex=False)
                df_copy.columns = cols
                print(f"     âœ… Fixed duplicate columns using pandas dedup_names")
            else:
                print(f"     âœ… No duplicate columns found")

            # Reindex to include all columns, filling missing with NaN
            try:
                df_reindexed = df_copy.reindex(columns=all_columns_list, fill_value=None)
                fixed_dfs.append(df_reindexed)
                print(f"     âœ… Reindexed from {len(df_copy.columns)} to {len(df_reindexed.columns)} columns")
            except Exception as e:
                print(f"     âŒ Reindexing failed: {e}")
                # Fallback: keep original dataframe
                fixed_dfs.append(df_copy)

        return fixed_dfs

    # Debug logging for reindex issues
    print(f"ğŸ” DEBUG: About to concat {len(all_data_list)} dataframes")
    for i, df in enumerate(all_data_list):
        print(f"   DataFrame {i+1}: shape={df.shape}, index_unique={df.index.is_unique}")
        print(f"   Columns: {len(df.columns)} columns")
        if df.columns.duplicated().any():
            print(f"   âš ï¸ Duplicate columns detected!")

    # Apply simple duplicate column fix before concat
    print(f"ğŸ”§ Applying simple duplicate column resolution...")

    # Step 1: Fix duplicate columns within each dataframe
    simple_fixed_list = []
    for i, df in enumerate(all_data_list):
        print(f"   Processing DataFrame {i+1}: {df.shape}")

        if df.columns.duplicated().any():
            print(f"     âš ï¸ Found {df.columns.duplicated().sum()} duplicate columns, fixing...")
            # Simple approach: use make_unique from pandas
            df_copy = df.copy()
            df_copy.columns = pd.io.common.dedup_names(df_copy.columns, is_potential_multiindex=False)
            print(f"     âœ… Fixed duplicate columns")
            simple_fixed_list.append(df_copy)
        else:
            print(f"     âœ… No duplicate columns")
            simple_fixed_list.append(df.copy())

    # Step 2: Try concat with ignore_index and sort=False
    try:
        print(f"ğŸ”§ Attempting concat with {len(simple_fixed_list)} dataframes...")
        merged_df = pd.concat(simple_fixed_list, ignore_index=True, sort=False)
        print(f"âœ… Concat successful: {merged_df.shape}")
    except Exception as e:
        print(f"âŒ CONCAT FAILED: {e}")

        # Fallback: Try with different concat options
        print(f"ğŸ”§ Trying fallback concat options...")
        try:
            # Reset index on all dataframes first
            reset_list = []
            for i, df in enumerate(simple_fixed_list):
                df_reset = df.reset_index(drop=True)
                reset_list.append(df_reset)

            merged_df = pd.concat(reset_list, ignore_index=True, sort=False)
            print(f"âœ… Fallback concat successful: {merged_df.shape}")
        except Exception as e2:
            print(f"âŒ Fallback concat also failed: {e2}")
            import traceback
            traceback.print_exc()
            raise

    logs.append(f"å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸã€‚åˆè¨ˆ: {len(merged_df)}ä»¶")

    if 'å›ç­”æ—¥æ™‚' in merged_df.columns:
        print(f"ğŸ” DEBUG: Processing datetime column")
        print(f"   merged_df shape before datetime: {merged_df.shape}")
        print(f"   Index unique before datetime: {merged_df.index.is_unique}")

        try:
            merged_df['å›ç­”æ—¥æ™‚'] = pd.to_datetime(merged_df['å›ç­”æ—¥æ™‚'], errors='coerce')
            print(f"âœ… Datetime conversion successful")
        except Exception as e:
            print(f"âŒ DATETIME CONVERSION FAILED: {e}")
            raise

        try:
            before_drop = len(merged_df)
            merged_df.dropna(subset=['å›ç­”æ—¥æ™‚'], inplace=True)
            after_drop = len(merged_df)
            print(f"âœ… dropna successful: {before_drop} -> {after_drop}")
        except Exception as e:
            print(f"âŒ DROPNA FAILED: {e}")
            import traceback
            traceback.print_exc()
            raise

        try:
            print(f"ğŸ” DEBUG: About to sort by å›ç­”æ—¥æ™‚")
            print(f"   Index before sort: unique={merged_df.index.is_unique}")
            merged_df.sort_values(by='å›ç­”æ—¥æ™‚', inplace=True)
            print(f"âœ… Sort successful")
        except Exception as e:
            print(f"âŒ SORT_VALUES FAILED: {e}")
            print("ğŸ¯ THIS MIGHT BE THE REINDEXING ERROR!")
            import traceback
            traceback.print_exc()
            raise

        logs.append(f"å›ç­”æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥ã®é›†è¨ˆ
    client_results = {}
    logs.append("--- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥é›†è¨ˆå‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: {len(data_files)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰, {len(all_data_list)} ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿")
    logs.append(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: merged_dfã¯åˆè¨ˆ {len(merged_df)} è¡Œ")
    
    for client_name, group in client_settings_df.groupby('ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå'):
        logs.append(f"'{client_name}' ã®é›†è¨ˆã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã‹ã‚‰è³ªå•ã‚’å–å¾—
        questions_to_aggregate = group['é›†è¨ˆå¯¾è±¡ã®è³ªå•æ–‡'].tolist()
        
        # å›ºå®šè³ªå•ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é™¤å¤–ï¼‰
        all_questions = list(dict.fromkeys(FIXED_QUESTIONS + questions_to_aggregate))
        logs.append(f"'{client_name}' ã«ã¯å›ºå®šè³ªå•ã‚’å«ã‚€åˆè¨ˆ {len(all_questions)} å€‹ã®è³ªå•ã‚’é›†è¨ˆã—ã¾ã™ã€‚")
        logs.append(f"'{client_name}' ã®å†…è¨³: å›ºå®šè³ªå•{len(FIXED_QUESTIONS)}å€‹ + ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè³ªå•{len(questions_to_aggregate)}å€‹")
        
        # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ã®ã¿ã‚’é¸æŠ
        cols_to_select = ['NO']
        found_questions = []
        not_found_questions = []
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰ã®è³ªå•ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        client_specific_questions = questions_to_aggregate  # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã‹ã‚‰ã®è³ªå•
        
        for q in all_questions:
            found = False
            columns_for_this_question = []
            
            # å®Œå…¨ä¸€è‡´ã®åˆ—ã‚’æ¢ã™
            if q in merged_df.columns:
                columns_for_this_question.append(q)
                found = True
            
            # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã®åˆ—ã‚’æ¢ã™ï¼ˆä¾‹: è³ªå•_1, è³ªå•_2ãªã©ï¼‰
            for col in merged_df.columns:
                if str(col).startswith(q + '_'):
                    columns_for_this_question.append(col)
                    found = True
            
            # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¿½åŠ 
            if found and q in all_questions:  # all_questionsã«ã¯å›ºå®šè³ªå•+ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè³ªå•ãŒå«ã¾ã‚Œã‚‹
                cols_to_select.extend(columns_for_this_question)
                found_questions.append(q)
            elif not found:
                not_found_questions.append(q)
        
        cols_to_select = list(dict.fromkeys(cols_to_select))  # é‡è¤‡ã‚’é™¤å»
        
        logs.append(f"'{client_name}' - è¦‹ã¤ã‹ã£ãŸè³ªå•: {len(found_questions)}/{len(all_questions)}")
        logs.append(f"'{client_name}' - é¸æŠã—ãŸåˆ—æ•°: {len(cols_to_select)}")
        logs.append(f"'{client_name}' - merged_dfã®ç·åˆ—æ•°: {len(merged_df.columns)}")
        
        if not_found_questions:
            logs.append(f"'{client_name}' - è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸè³ªå•: {len(not_found_questions)} å€‹")
            for q in not_found_questions[:3]:
                logs.append(f"  - {q[:50]}...")
            
        # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã«ã©ã®è³ªå•ãŒè¦‹ã¤ã‹ã£ãŸã‹
        logs.append(f"'{client_name}' - ãƒ‡ãƒãƒƒã‚°: è¦‹ã¤ã‹ã£ãŸè³ªå•ã®ä¾‹:")
        for q in found_questions[:3]:
            logs.append(f"  âœ“ {q[:50]}...")
            
        # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã«é¸æŠã•ã‚ŒãŸcolumnsã®ä¾‹
        logs.append(f"'{client_name}' - ãƒ‡ãƒãƒƒã‚°: é¸æŠã•ã‚ŒãŸåˆ—ã®ä¾‹:")
        for col in cols_to_select[1:6]:  # NOä»¥å¤–ã®æœ€åˆã®5åˆ—
            logs.append(f"  â†’ {col[:50]}...")
            
        # ãƒ‡ãƒãƒƒã‚°: ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰ã®è³ªå•ãŒã‚ã‚‹ã‹
        client_unique_questions = [q for q in questions_to_aggregate if q not in FIXED_QUESTIONS]
        logs.append(f"'{client_name}' - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰è³ªå•: {len(client_unique_questions)}å€‹")
        for q in client_unique_questions[:2]:
            logs.append(f"  ğŸ”¹ {q[:50]}...")
        
        if 'å›ç­”æ—¥æ™‚' in merged_df.columns:
            cols_to_select.append('å›ç­”æ—¥æ™‚')
        
        if len(cols_to_select) <= 1:
            logs.append(f"'{client_name}' ã®é›†è¨ˆå¯¾è±¡ã®è³ªå•ãŒãƒ‡ãƒ¼ã‚¿å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue
            
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠï¼ˆå…¨762ä»¶ã‹ã‚‰å¿…è¦ãªåˆ—ã®ã¿ï¼‰
        print(f"ğŸ” DEBUG: Selecting client data for {client_name}")
        print(f"   Columns to select: {len(cols_to_select)}")
        print(f"   merged_df shape: {merged_df.shape}")
        print(f"   merged_df index unique: {merged_df.index.is_unique}")

        try:
            client_data = merged_df[cols_to_select]
            print(f"âœ… Client data selection successful: {client_data.shape}")
        except Exception as e:
            print(f"âŒ CLIENT DATA SELECTION FAILED: {e}")
            print("ğŸ¯ THIS MIGHT BE THE REINDEXING ERROR!")
            import traceback
            traceback.print_exc()
            raise
        
        # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        logs.append(f"'{client_name}' å°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆä¸­...")
        
        base_mapping_df = pd.DataFrame()
        text_to_q_map = {}
        
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åé›†ï¼ˆå›ºå®šè³ªå•ã‚’å«ã‚€å…¨è³ªå•ã‚’ç¢ºå®Ÿã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        for file_col in question_master_df.columns:
            if file_col != 'è³ªå•æ–‡' and file_col.endswith('.xlsx'):
                temp_mapping = question_master_df[['è³ªå•æ–‡', file_col]].dropna()
                for _, row in temp_mapping.iterrows():
                    if row['è³ªå•æ–‡'] not in text_to_q_map:
                        text_to_q_map[row['è³ªå•æ–‡']] = row[file_col]
        
        # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ãƒªã‚¹ãƒˆã‹ã‚‰ã€è©²å½“ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°ã®ã¿ã‚’æŠ½å‡º
        client_mapping_data = []
        
        for question_text in all_questions:
            # question_masterã§ã“ã®è³ªå•ã‚’æ¢ã™
            matching_rows = question_master_df[question_master_df['è³ªå•æ–‡'] == question_text]
            if not matching_rows.empty:
                row = matching_rows.iloc[0]
                # ã“ã®è³ªå•ã«å¯¾ã—ã¦æœ€åˆã«è¦‹ã¤ã‹ã£ãŸéNULLã®mappingã‚’ä½¿ç”¨
                for col in question_master_df.columns:
                    if col.endswith('.xlsx') and pd.notna(row[col]):
                        client_mapping_data.append({
                            'è³ªå•æ–‡': question_text,
                            'è³ªå•ç•ªå·': row[col]
                        })
                        break
        
        base_mapping_df = pd.DataFrame(client_mapping_data)
        logs.append(f"'{client_name}' ã®ãƒãƒƒãƒ”ãƒ³ã‚°: {len(base_mapping_df)}å€‹ã®è³ªå•")

        # client_dataã®åˆ—åã‚’è³ªå•æ–‡ã‹ã‚‰è³ªå•ç•ªå·ã¸å†å¤‰æ›ï¼ˆFAåˆ—ã‚‚è€ƒæ…®ï¼‰
        final_rename_map = {}
        for col_name in client_data.columns:
            # FAåˆ—ãªã©ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒä»˜ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_suffixed = False
            for q_text, q_num in text_to_q_map.items():
                if str(col_name).startswith(q_text + '_'):
                    suffix = str(col_name).replace(q_text, '')
                    final_rename_map[col_name] = q_num + suffix
                    is_suffixed = True
                    break
            # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒãªãã€å®Œå…¨ä¸€è‡´ã™ã‚‹å ´åˆ
            if not is_suffixed and col_name in text_to_q_map:
                final_rename_map[col_name] = text_to_q_map[col_name]

        output_client_data = client_data.rename(columns=final_rename_map)
        
        client_results[client_name] = {
            'data': output_client_data,
            'base_file': f"{client_name}å°‚ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°",
            'mapping': base_mapping_df
        }
        
        logs.append(f"'{client_name}' ã®é›†è¨ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¿½åŠ 
    logs.append("=== ã‚µãƒãƒªãƒ¼ ===")
    logs.append(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files)}")
    logs.append(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_data_list)}")
    logs.append(f"merged_dfç·è¡Œæ•°: {len(merged_df)}")
    logs.append(f"å‡¦ç†æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°: {len(client_results)}")
    for i, df in enumerate(all_data_list):
        logs.append(f"  ãƒ•ã‚¡ã‚¤ãƒ«{i+1}ã®å¯„ä¸è¡Œæ•°: {len(df)} è¡Œ")
    
    return client_results, merged_df, logs