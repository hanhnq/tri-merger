import pandas as pd
import io
from datetime import datetime
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

# å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å…±é€šã§å«ã¾ã‚Œã‚‹å›ºå®šè³ªå•
FIXED_QUESTIONS = [
    'ã‚ãªãŸã®å¹´ä»£æ€§åˆ¥ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸãŒãŠä½ã¾ã„ã®éƒ½é“åºœçœŒã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚',
    'å®¶æ—æ§‹æˆã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã®å¹´åã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ç¤¾ä¼šäººçµŒé¨“ã¯ä½•å¹´é–“ã§ã™ã‹ï¼Ÿ',
    'æœ€çµ‚å­¦æ­´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã®ãŠä½ã„ã®å®¶è³ƒã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',
    'ã‚ãªãŸã«å½“ã¦ã¯ã¾ã‚‹é¸æŠè‚¢ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚'
]


def aggregate_data(data_files, question_master_df, client_settings_df):
    """
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã«åŸºã¥ãã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆã—ã€
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã”ã¨ã«å€‹åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦è¿”ã™ã€‚
    
    Args:
        data_files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        question_master_df: è³ªå•ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        client_settings_df: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
        dict: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        pandas.DataFrame: ä¸­é–“ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨çµåˆãƒ‡ãƒ¼ã‚¿ï¼‰
        list: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    # Debug print to console and file
    debug_msg = f"\n=== AGGREGATE_DATA CALLED ===\n"
    debug_msg += f"Number of data_files received: {len(data_files)}\n"
    for i, f in enumerate(data_files):
        file_size = getattr(f, 'size', 'unknown')
        debug_msg += f"  File {i+1}: {f.name} (size: {file_size} bytes)\n"
    debug_msg += "=" * 30 + "\n"
    
    print(debug_msg)
    
    # Also write to file for debugging
    with open(r"D:\python\tri-merger\debug_log.txt", "w", encoding="utf-8") as f:
        f.write(debug_msg)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    if not data_files:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å°‘ãªãã¨ã‚‚1ã¤ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    
    logging.info(f"Received {len(data_files)} data files for aggregation")
    logs = []
    all_data_list = []
    
    logs.append("--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å¤‰æ›å‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files)}")
    for i, f in enumerate(data_files):
        file_size = getattr(f, 'size', 'unknown')
        logs.append(f"  {i+1}. {f.name} (ã‚µã‚¤ã‚º: {file_size} bytes)")
    
    logs.append(f"question_masterã‹ã‚‰æ¤œå‡ºã•ã‚ŒãŸç·è³ªå•æ•°: {len(question_master_df)}å€‹")
    logs.append("ãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã®è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ãŒç‹¬è‡ªã®è³ªå•ã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æŒã¤ãŸã‚ï¼‰")
    print(f"ğŸ” DEBUG: Starting to process {len(data_files)} files")

    for uploaded_file in data_files:
        print(f"ğŸ” DEBUG: Processing file: {uploaded_file.name}")

        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ–‡å­—åŒ–ã‘å¯¾ç­–ï¼ˆquestion_master.pyã¨åŒã˜å‡¦ç†ï¼‰
        original_filename = uploaded_file.name
        filename = original_filename

        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
        logging.info(f"Original filename in aggregation: {repr(original_filename)}")
        print(f"ğŸ” DEBUG: Original filename: {repr(original_filename)}")
        
        # æ–‡å­—åŒ–ã‘ã®æ¤œå‡ºã¨ä¿®æ­£
        try:
            # ä¸€èˆ¬çš„ãªæ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if any(ord(c) > 127 and ord(c) < 256 for c in filename):
                # Latin-1ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆ
                try:
                    filename = filename.encode('latin-1').decode('utf-8')
                except:
                    pass
            
            # ãã‚Œã§ã‚‚æ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹å ´åˆã¯ã€å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            if 'ï¿½' in filename or any(ord(c) > 0xFFFF for c in filename):
                import hashlib
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ç”¨
                file_hash = hashlib.md5(uploaded_file.name.encode('utf-8', errors='ignore')).hexdigest()[:8]
                filename = f"file_{file_hash}.xlsx"
                logging.warning(f"Filename contains invalid characters in aggregation. Using safe name: {filename}")
        except Exception:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä½¿ç”¨
            import time
            filename = f"file_{int(time.time())}.xlsx"
            
        if filename.endswith('.xlsx') and not filename.startswith('~'):
            logs.append(f"'{filename}' ã®å‡¦ç†ã‚’é–‹å§‹...")
            
            # question_master_dfã®åˆ—ã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã®åˆ—ã‚’æ¢ã™
            # æ–‡å­—åŒ–ã‘ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã¨ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
            file_column = None
            q_to_text_map = {}

            # 1. å®Œå…¨ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
            if filename in question_master_df.columns:
                file_column = filename
            elif original_filename in question_master_df.columns:
                file_column = original_filename
            # 2. Plus_ãƒãƒ¼ã‚¸å®Œæˆãƒ‡ãƒ¼ã‚¿.xlsx ã®ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹
            elif "Plus_ãƒãƒ¼ã‚¸å®Œæˆãƒ‡ãƒ¼ã‚¿" in filename:
                for col in question_master_df.columns:
                    if "Plus2" in col and not col.startswith("[ã‚³ãƒ”ãƒ¼]"):
                        file_column = col
                        break
            # 3. éƒ¨åˆ†ä¸€è‡´ã§ã®æ¤œç´¢ï¼ˆfallbackï¼‰
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸»è¦éƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦æ¤œç´¢
                clean_filename = filename.replace(".xlsx", "")
                for col in question_master_df.columns:
                    if col.endswith('.xlsx') and clean_filename in col:
                        file_column = col
                        break
            
            if file_column:
                file_mapping = question_master_df[['è³ªå•æ–‡', file_column]].dropna()
                q_to_text_map = dict(zip(file_mapping[file_column], file_mapping['è³ªå•æ–‡']))
                logs.append(f"'{filename}' ã®è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—ã—ã¾ã—ãŸã€‚({len(q_to_text_map)}å€‹ã®è³ªå•)")
            else:
                # å…·ä½“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹
                # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åé›†
                logs.append(f"'{filename}' (å…ƒ: '{original_filename}') ã«å¯¾å¿œã™ã‚‹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                logs.append(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {[col for col in question_master_df.columns if col.endswith('.xlsx')]}")
                
                # æ±ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã—ã¦æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨
                first_file_col = None
                for col in question_master_df.columns:
                    if col != 'è³ªå•æ–‡' and col.endswith('.xlsx'):
                        first_file_col = col
                        break
                
                if first_file_col:
                    temp_mapping = question_master_df[['è³ªå•æ–‡', first_file_col]].dropna()
                    q_to_text_map = dict(zip(temp_mapping[first_file_col], temp_mapping['è³ªå•æ–‡']))
                    logs.append(f"'{filename}' ã§ã¯ '{first_file_col}' ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä»£æ›¿ä½¿ç”¨ã—ã¾ã™ã€‚({len(q_to_text_map)}å€‹ã®è³ªå•)")
                else:
                    logs.append(f"'{filename}' ã§ã¯åˆ©ç”¨å¯èƒ½ãªãƒãƒƒãƒ”ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ƒã®åˆ—åã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                
                logging.warning(f"File column not found for {filename} or {original_filename}. Using generic mapping from {first_file_col}.")
                logging.info(f"Available columns: {list(question_master_df.columns)}")
            
            try:
                print(f"ğŸ” DEBUG: About to read Excel file: {filename}")
                df_data = pd.read_excel(uploaded_file, sheet_name='data')
                print(f"âœ… DEBUG: Successfully read Excel: {len(df_data)} rows, {len(df_data.columns)} columns")
                logs.append(f"'{filename}' ã®dataã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€‚å…ƒãƒ‡ãƒ¼ã‚¿: {len(df_data)}è¡Œ, {len(df_data.columns)}åˆ—")
                
                if df_data.empty:
                    logs.append(f"'{filename}' ã®dataã‚·ãƒ¼ãƒˆã¯ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®åˆ—ã‚’rename
                new_columns = {}
                mapped_count = 0
                for col in df_data.columns:
                    if col in q_to_text_map:
                        new_columns[col] = q_to_text_map[col]
                        mapped_count += 1
                    else:
                        # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä¾‹: Q-001_1, Q-001_2ãªã©ï¼‰
                        for q_num, q_text in q_to_text_map.items():
                            if str(col).startswith(q_num + '_'):
                                suffix = str(col).replace(q_num, '')
                                new_columns[col] = f"{q_text}{suffix}"
                                mapped_count += 1
                                break
                
                logs.append(f"'{filename}' ã§ {mapped_count}/{len(df_data.columns)} åˆ—ãŒãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸã€‚")
                df_data.rename(columns=new_columns, inplace=True)
                
                all_data_list.append(df_data)
                logs.append(f"'{filename}' ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†ã€‚({len(df_data)}ä»¶) -> all_data_liståˆè¨ˆ: {len(all_data_list)}ãƒ•ã‚¡ã‚¤ãƒ«")

            except Exception as e:
                print(f"âŒ DEBUG: Error processing {filename}: {e}")
                import traceback
                error_traceback = traceback.format_exc()
                print(f"âŒ DEBUG: Full traceback:\n{error_traceback}")
                logs.append(f"'{filename}' ã®ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                logs.append(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_traceback}")

    print(f"ğŸ” DEBUG: Checking data files processing results")
    print(f"   all_data_list length: {len(all_data_list)}")
    print(f"   Original data_files count: {len(data_files)}")

    if not all_data_list:
        print(f"âŒ NO DATA FOUND! This is the error, not reindexing!")
        print(f"   Files were processed but no data sheets were successfully read")
        print(f"   Check the logs above for file processing errors")
        raise ValueError("é›†è¨ˆå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    logs.append("--- å…¨ãƒ‡ãƒ¼ã‚¿ã®çµåˆå‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"çµåˆå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_data_list)}")
    for i, df in enumerate(all_data_list):
        logs.append(f"  ãƒ•ã‚¡ã‚¤ãƒ«{i+1}: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    
    # ğŸ¯ Smart Row Stacking: Handle same-structure survey files properly
    print(f"ğŸ”§ Smart stacking {len(all_data_list)} survey dataframes...")

    if len(all_data_list) == 1:
        merged_df = all_data_list[0].copy()
        print(f"âœ… Single file: {merged_df.shape}")
    else:
        # For multiple files with same structure, just stack rows vertically
        # This is the correct approach for survey data from different days/batches
        try:
            merged_df = pd.concat(all_data_list, ignore_index=True, sort=False)
            print(f"âœ… Row stacking successful: {merged_df.shape}")
            print(f"   Combined {len(all_data_list)} files with {len(merged_df)} total rows")
        except Exception as e:
            print(f"âŒ Row stacking failed: {e}")
            # If concat fails, there might be column mismatch - align columns first
            print(f"ğŸ”§ Aligning columns before stacking...")

            # Get all unique columns across all dataframes
            all_columns = set()
            for df in all_data_list:
                all_columns.update(df.columns)

            # Align all dataframes to have same columns (handle duplicates)
            aligned_dfs = []
            all_columns_list = list(all_columns)

            for i, df in enumerate(all_data_list):
                # Check for duplicate columns in source DataFrame
                if df.columns.duplicated().any():
                    print(f"   âš ï¸  DataFrame {i+1} has duplicate columns, cleaning...")
                    # Remove duplicate columns by keeping only the first occurrence
                    df = df.loc[:, ~df.columns.duplicated()]

                # Safe reindex with duplicate handling
                try:
                    aligned_df = df.reindex(columns=all_columns_list, fill_value=None)
                    aligned_dfs.append(aligned_df)
                    print(f"   Aligned DataFrame {i+1}: {aligned_df.shape}")
                except Exception as e:
                    print(f"   âŒ Failed to align DataFrame {i+1}: {e}")
                    # Fallback: just use the original DataFrame
                    aligned_dfs.append(df)
                    print(f"   Using original DataFrame {i+1}: {df.shape}")

            # Now try concat again
            merged_df = pd.concat(aligned_dfs, ignore_index=True, sort=False)
            print(f"âœ… Aligned row stacking successful: {merged_df.shape}")

    logs.append(f"å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸã€‚åˆè¨ˆ: {len(merged_df)}ä»¶")

    # ğŸ”§ Column Sorting: åˆ—ã‚’æ­£ã—ã„é †åºã«ä¸¦ã³æ›¿ãˆï¼ˆNatural sortingï¼‰
    print(f"ğŸ”§ Sorting columns in proper natural order...")

    import re

    def natural_sort_key(col_name):
        """Natural sorting - æ•°å€¤ã‚’æ­£ã—ã„é †åºã§ã‚½ãƒ¼ãƒˆ"""
        col_str = str(col_name)

        # ç‰¹åˆ¥ãªåˆ—ã¯æœ€åˆã¨æœ€å¾Œã«é…ç½®
        if col_str == 'NO':
            return (0, [])
        elif col_str == 'å›ç­”æ—¥æ™‚':
            return (9999, [])

        # Q-ã§å§‹ã¾ã‚‹åˆ—ã¯å„ªå…ˆçš„ã«é…ç½®
        if col_str.startswith('Q-'):
            priority = 1
        else:
            priority = 2

        # æ•°å€¤ã¨æ–‡å­—åˆ—ã‚’åˆ†é›¢ã—ã¦ã‚½ãƒ¼ãƒˆ
        def try_int(text):
            return int(text) if text.isdigit() else text

        parts = [try_int(c) for c in re.split(r'(\d+)', col_str)]
        return (priority, parts)

    # åˆ—ã‚’ä¸¦ã³æ›¿ãˆ
    sorted_columns = sorted(merged_df.columns, key=natural_sort_key)
    merged_df = merged_df[sorted_columns]

    print(f"âœ… Natural column sorting completed. Column order example:")
    q011_cols = [col for col in sorted_columns if str(col).startswith('Q-011')]
    if q011_cols:
        print(f"   Q-011 columns: {q011_cols[:10]}...")  # Show first 10

    logs.append(f"åˆ—ã‚’æ­£ã—ã„è‡ªç„¶é †åºã«ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")

    if 'å›ç­”æ—¥æ™‚' in merged_df.columns:
        print(f"ğŸ” DEBUG: Processing datetime column")
        print(f"   merged_df shape before datetime: {merged_df.shape}")
        print(f"   Index unique before datetime: {merged_df.index.is_unique}")

        try:
            merged_df['å›ç­”æ—¥æ™‚'] = pd.to_datetime(merged_df['å›ç­”æ—¥æ™‚'], errors='coerce')
            print(f"âœ… Datetime conversion successful")
        except Exception as e:
            print(f"âŒ DATETIME CONVERSION FAILED: {e}")
            raise

        try:
            before_drop = len(merged_df)
            merged_df.dropna(subset=['å›ç­”æ—¥æ™‚'], inplace=True)
            after_drop = len(merged_df)
            print(f"âœ… dropna successful: {before_drop} -> {after_drop}")
        except Exception as e:
            print(f"âŒ DROPNA FAILED: {e}")
            import traceback
            traceback.print_exc()
            raise

        try:
            print(f"ğŸ” DEBUG: About to sort by å›ç­”æ—¥æ™‚")
            print(f"   Index before sort: unique={merged_df.index.is_unique}")
            merged_df.sort_values(by='å›ç­”æ—¥æ™‚', inplace=True)
            print(f"âœ… Sort successful")
        except Exception as e:
            print(f"âŒ SORT_VALUES FAILED: {e}")
            print("ğŸ¯ THIS MIGHT BE THE REINDEXING ERROR!")
            import traceback
            traceback.print_exc()
            raise

        logs.append(f"å›ç­”æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥ã®é›†è¨ˆ
    client_results = {}
    logs.append("--- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥é›†è¨ˆå‡¦ç†ã‚’é–‹å§‹ ---")
    logs.append(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: {len(data_files)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰, {len(all_data_list)} ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿")
    logs.append(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: merged_dfã¯åˆè¨ˆ {len(merged_df)} è¡Œ")
    
    for client_name, group in client_settings_df.groupby('ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå'):
        logs.append(f"'{client_name}' ã®é›†è¨ˆã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã‹ã‚‰è³ªå•ã‚’å–å¾—
        questions_to_aggregate = group['é›†è¨ˆå¯¾è±¡ã®è³ªå•æ–‡'].tolist()
        
        # å›ºå®šè³ªå•ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é™¤å¤–ï¼‰
        all_questions = list(dict.fromkeys(FIXED_QUESTIONS + questions_to_aggregate))
        logs.append(f"'{client_name}' ã«ã¯å›ºå®šè³ªå•ã‚’å«ã‚€åˆè¨ˆ {len(all_questions)} å€‹ã®è³ªå•ã‚’é›†è¨ˆã—ã¾ã™ã€‚")
        logs.append(f"'{client_name}' ã®å†…è¨³: å›ºå®šè³ªå•{len(FIXED_QUESTIONS)}å€‹ + ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè³ªå•{len(questions_to_aggregate)}å€‹")
        
        # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ã®ã¿ã‚’é¸æŠ
        cols_to_select = ['NO']
        found_questions = []
        not_found_questions = []
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰ã®è³ªå•ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        client_specific_questions = questions_to_aggregate  # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šã‹ã‚‰ã®è³ªå•
        
        for q in all_questions:
            found = False
            columns_for_this_question = []
            
            # å®Œå…¨ä¸€è‡´ã®åˆ—ã‚’æ¢ã™
            if q in merged_df.columns:
                columns_for_this_question.append(q)
                found = True
            
            # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã®åˆ—ã‚’æ¢ã™ï¼ˆä¾‹: è³ªå•_1, è³ªå•_2ãªã©ï¼‰
            for col in merged_df.columns:
                if str(col).startswith(q + '_'):
                    columns_for_this_question.append(col)
                    found = True
            
            # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¿½åŠ 
            if found and q in all_questions:  # all_questionsã«ã¯å›ºå®šè³ªå•+ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè³ªå•ãŒå«ã¾ã‚Œã‚‹
                cols_to_select.extend(columns_for_this_question)
                found_questions.append(q)
            elif not found:
                not_found_questions.append(q)
        
        cols_to_select = list(dict.fromkeys(cols_to_select))  # é‡è¤‡ã‚’é™¤å»
        
        logs.append(f"'{client_name}' - è¦‹ã¤ã‹ã£ãŸè³ªå•: {len(found_questions)}/{len(all_questions)}")
        logs.append(f"'{client_name}' - é¸æŠã—ãŸåˆ—æ•°: {len(cols_to_select)}")
        logs.append(f"'{client_name}' - merged_dfã®ç·åˆ—æ•°: {len(merged_df.columns)}")
        
        if not_found_questions:
            logs.append(f"'{client_name}' - è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸè³ªå•: {len(not_found_questions)} å€‹")
            for q in not_found_questions[:3]:
                logs.append(f"  - {q[:50]}...")
            
        # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã«ã©ã®è³ªå•ãŒè¦‹ã¤ã‹ã£ãŸã‹
        logs.append(f"'{client_name}' - ãƒ‡ãƒãƒƒã‚°: è¦‹ã¤ã‹ã£ãŸè³ªå•ã®ä¾‹:")
        for q in found_questions[:3]:
            logs.append(f"  âœ“ {q[:50]}...")
            
        # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã«é¸æŠã•ã‚ŒãŸcolumnsã®ä¾‹
        logs.append(f"'{client_name}' - ãƒ‡ãƒãƒƒã‚°: é¸æŠã•ã‚ŒãŸåˆ—ã®ä¾‹:")
        for col in cols_to_select[1:6]:  # NOä»¥å¤–ã®æœ€åˆã®5åˆ—
            logs.append(f"  â†’ {col[:50]}...")
            
        # ãƒ‡ãƒãƒƒã‚°: ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰ã®è³ªå•ãŒã‚ã‚‹ã‹
        client_unique_questions = [q for q in questions_to_aggregate if q not in FIXED_QUESTIONS]
        logs.append(f"'{client_name}' - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰è³ªå•: {len(client_unique_questions)}å€‹")
        for q in client_unique_questions[:2]:
            logs.append(f"  ğŸ”¹ {q[:50]}...")
        
        if 'å›ç­”æ—¥æ™‚' in merged_df.columns:
            cols_to_select.append('å›ç­”æ—¥æ™‚')
        
        if len(cols_to_select) <= 1:
            logs.append(f"'{client_name}' ã®é›†è¨ˆå¯¾è±¡ã®è³ªå•ãŒãƒ‡ãƒ¼ã‚¿å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue
            
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠï¼ˆå…¨762ä»¶ã‹ã‚‰å¿…è¦ãªåˆ—ã®ã¿ï¼‰
        print(f"ğŸ” DEBUG: Selecting client data for {client_name}")
        print(f"   Columns to select: {len(cols_to_select)}")
        print(f"   merged_df shape: {merged_df.shape}")
        print(f"   merged_df index unique: {merged_df.index.is_unique}")

        try:
            client_data = merged_df[cols_to_select]
            print(f"âœ… Client data selection successful: {client_data.shape}")
        except Exception as e:
            print(f"âŒ CLIENT DATA SELECTION FAILED: {e}")
            print("ğŸ¯ THIS MIGHT BE THE REINDEXING ERROR!")
            import traceback
            traceback.print_exc()
            raise
        
        # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        logs.append(f"'{client_name}' å°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆä¸­...")
        
        base_mapping_df = pd.DataFrame()
        text_to_q_map = {}
        
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è³ªå•ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åé›†ï¼ˆå›ºå®šè³ªå•ã‚’å«ã‚€å…¨è³ªå•ã‚’ç¢ºå®Ÿã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        for file_col in question_master_df.columns:
            if file_col != 'è³ªå•æ–‡' and file_col.endswith('.xlsx'):
                temp_mapping = question_master_df[['è³ªå•æ–‡', file_col]].dropna()
                for _, row in temp_mapping.iterrows():
                    if row['è³ªå•æ–‡'] not in text_to_q_map:
                        text_to_q_map[row['è³ªå•æ–‡']] = row[file_col]
        
        # ã“ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è³ªå•ãƒªã‚¹ãƒˆã‹ã‚‰ã€è©²å½“ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°ã®ã¿ã‚’æŠ½å‡º
        client_mapping_data = []

        for question_text in all_questions:
            # question_masterã§ã“ã®è³ªå•ã‚’æ¢ã™
            matching_rows = question_master_df[question_master_df['è³ªå•æ–‡'] == question_text]
            if not matching_rows.empty:
                row = matching_rows.iloc[0]
                # ã“ã®è³ªå•ã«å¯¾ã—ã¦æœ€åˆã«è¦‹ã¤ã‹ã£ãŸéNULLã®mappingã‚’ä½¿ç”¨
                for col in question_master_df.columns:
                    if col.endswith('.xlsx') and pd.notna(row[col]):
                        client_mapping_data.append({
                            'è³ªå•æ–‡': question_text,
                            'è³ªå•ç•ªå·': row[col]
                        })
                        break

        base_mapping_df = pd.DataFrame(client_mapping_data)
        logs.append(f"'{client_name}' ã®ãƒãƒƒãƒ”ãƒ³ã‚°: {len(base_mapping_df)}å€‹ã®è³ªå•")

        # client_dataã®åˆ—åã‚’è³ªå•æ–‡ã‹ã‚‰è³ªå•ç•ªå·ã¸å†å¤‰æ›ï¼ˆFAåˆ—ã‚‚è€ƒæ…®ï¼‰
        final_rename_map = {}
        for col_name in client_data.columns:
            # FAåˆ—ãªã©ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒä»˜ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_suffixed = False
            for q_text, q_num in text_to_q_map.items():
                if str(col_name).startswith(q_text + '_'):
                    suffix = str(col_name).replace(q_text, '')
                    final_rename_map[col_name] = q_num + suffix
                    is_suffixed = True
                    break
            # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒãªãã€å®Œå…¨ä¸€è‡´ã™ã‚‹å ´åˆ
            if not is_suffixed and col_name in text_to_q_map:
                final_rename_map[col_name] = text_to_q_map[col_name]

        output_client_data = client_data.rename(columns=final_rename_map)
        
        client_results[client_name] = {
            'data': output_client_data,
            'base_file': f"{client_name}å°‚ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°",
            'mapping': base_mapping_df
        }
        
        logs.append(f"'{client_name}' ã®é›†è¨ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¿½åŠ 
    logs.append("=== ã‚µãƒãƒªãƒ¼ ===")
    logs.append(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files)}")
    logs.append(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_data_list)}")
    logs.append(f"merged_dfç·è¡Œæ•°: {len(merged_df)}")
    logs.append(f"å‡¦ç†æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°: {len(client_results)}")
    for i, df in enumerate(all_data_list):
        logs.append(f"  ãƒ•ã‚¡ã‚¤ãƒ«{i+1}ã®å¯„ä¸è¡Œæ•°: {len(df)} è¡Œ")
    
    return client_results, merged_df, logs